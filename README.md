# OmniScale â€” High-Performance Neural Recommender & Logistics Optimizer

<p align="center">
  <a href="https://colab.research.google.com/drive/"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Google Colab"></a>
  <a href="https://pytorch.org/"><img src="https://img.shields.io/badge/PyTorch-2.0+-red?logo=pytorch" alt="PyTorch"></a>
  <a href="https://www.python.org/"><img src="https://img.shields.io/badge/Python-3.8+-blue?logo=python" alt="Python"></a>
  <a href="https://isocpp.org/"><img src="https://img.shields.io/badge/C++-11+-00599C?logo=c%2B%2B" alt="C++"></a>
</p>

> A scalable, high-performance system that combines **Neural Collaborative Filtering** for product recommendations with **Logistics Optimization** for warehouse order assignment â€” powered by C++ cores and distributed computing.

---

## ğŸš€ Overview

**OmniScale** is an end-to-end machine learning system designed for two critical e-commerce tasks:

1. **Neural Recommender** â€” Predicts user preferences using Deep Learning (NCF model)
2. **Logistics Optimizer** â€” Assigns orders to warehouses to minimize delivery distance under capacity constraints

The project demonstrates a complete MLOps pipeline: from raw data parsing â†’ feature engineering â†’ model training â†’ high-performance optimization â†’ distributed execution.

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        OmniScale Pipeline                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Phase 1:     â”‚ â†’  â”‚ Phase 2:     â”‚ â†’  â”‚ Phase 3:        â”‚  â”‚
â”‚  â”‚ Data Parsing â”‚    â”‚ Feature     â”‚    â”‚ Neural          â”‚  â”‚
â”‚  â”‚ & Cleaning   â”‚    â”‚ Mining       â”‚    â”‚ Recommender     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â†“                   â†“                    â†“              â”‚
â”‚  Stream Amazon       K-Means++           PyTorch NCF           â”‚
â”‚  JSON Reviews       Clustering          Model                 â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚  â”‚ Phase 4:     â”‚ â†’  â”‚ Phase 5:     â”‚                          â”‚
â”‚  â”‚ HPC          â”‚    â”‚ Distributed  â”‚                          â”‚
â”‚  â”‚ Optimizer    â”‚    â”‚ Computing    â”‚                          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚         â†“                   â†“                                    â”‚
â”‚  C++ / OpenMP         MapReduce                                â”‚
â”‚  Solver               Simulation                                â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Project Structure

```
OmniScale-Optimizer/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Raw Amazon reviews data
â”‚   â””â”€â”€ processed/              # Cleaned & feature-engineered data
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ parser/
â”‚   â”‚   â”œâ”€â”€ stream_parser.py   # JSON stream parser
â”‚   â”‚   â””â”€â”€ feature_miner.py   # Feature extraction & K-Means++
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ ncf_model.py       # Neural Collaborative Filtering
â”‚   â”œâ”€â”€ optimizer/
â”‚   â”‚   â”œâ”€â”€ solver.py          # Python logistics solver
â”‚   â”‚   â””â”€â”€ cpp_core/
â”‚   â”‚       â”œâ”€â”€ optimizer.cpp  # C++ HPC kernel
â”‚   â”‚       â””â”€â”€ fast_optimizer.so  # Compiled pybind11 module
â”‚   â””â”€â”€ distributed/
â”‚       â””â”€â”€ map_reduce_ops.py  # Distributed MapReduce simulation
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ OmniScale-Optimizer.ipynb  # Main execution notebook
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ (utility scripts)
â””â”€â”€ README.md
```

---

## âœ¨ Key Features

### ğŸ§  Neural Collaborative Filtering (NCF)
- **Deep Learning** recommender using PyTorch
- Embedding layers for users and items
- Multi-layer perceptron (MLP) for non-linear feature interactions
- MSE loss with Adam optimizer

### ğŸ“¦ Logistics Optimization
- **K-Means++** clustering for delivery zone discovery
- Greedy assignment with capacity constraints
- Minimizes total distance from customers to warehouses

### âš¡ High-Performance Computing (HPC)
- **C++ core** with OpenMP parallelization
- **pybind11** bindings for Python integration
- GPU-ready architecture (CUDA support)

### ğŸŒ Distributed Computing
- **MapReduce** simulation using `ProcessPoolExecutor`
- Parallel order assignment across multiple workers
- Scalable to real cluster deployments (AWS EC2, GCP)

---

## ğŸ“Š Usage

### Phase 1: Data Parsing
```
python
from src.parser.stream_parser import stream_amazon_data

# Stream JSON reviews one at a time
for record in stream_amazon_data('data/raw/reviews_Electronics_5.json'):
    print(record)
```

### Phase 2: Feature Mining
```
python
from src.parser.feature_miner import FeatureMiner

miner = FeatureMiner('data/raw/reviews_Electronics_5.json')
df = miner.extract_interactions(limit=100000)

# Cluster users into 10 delivery zones
centroids, labels = miner.manual_kmeans(df[['lat', 'lon']].values, k=10)
```

### Phase 3: Train Recommender
```
python
from src.models.ncf_model import NCFModel
import torch.optim as optim

model = NCFModel(num_users=5000, num_items=10000, embed_size=32)
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop...
```

### Phase 4: HPC Optimization
```
python
from src.optimizer.solver import LogisticsOptimizer

optimizer = LogisticsOptimizer(warehouse_locations, capacities)
assignments, usage = optimizer.assign_orders(user_locations)
```

### Phase 5: Distributed Execution
```
python
from src.distributed.map_reduce_ops import run_distributed_optimizer

assignments, usage = run_distributed_optimizer(
    df, warehouse_coords, caps, num_workers=4
)
```

---

## ğŸ“ˆ Performance

| Component | Metric | Value |
|-----------|--------|-------|
| NCF Model | Embedding Size | 32 |
| NCF Model | Training Epochs | 5 |
| K-Means++ | Clusters (Zones) | 10 |
| C++ Optimizer | Parallelization | OpenMP |
| Distributed | Workers | 4 (simulated) |

---

## ğŸ“š Technologies Used

- **Python 3.8+** â€” Core language
- **PyTorch 2.0+** â€” Neural network framework
- **NumPy** â€” Numerical computing
- **Pandas** â€” Data manipulation
- **pybind11** â€” Python/C++ bindings
- **C++11** â€” High-performance core
- **OpenMP** â€” Parallel computing
- **Google Colab** â€” Cloud execution environment

---

## ğŸ“ License

MIT License â€” See [LICENSE](LICENSE) for details.

---

<p align="center">
  <strong>OmniScale</strong> â€” Scaling Intelligence from Recommendation to Delivery
</p>
