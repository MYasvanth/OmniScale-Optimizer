{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dc827d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Current Working Directory: /content/drive/MyDrive/OmniScale-Optimizer\n",
      "Python Path updated. You can now import from 'src'.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "# 1. Mount the drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 2. Define the EXACT path to your project root\n",
    "project_root = \"/content/drive/MyDrive/OmniScale-Optimizer\"\n",
    "\n",
    "# 3. Add to sys.path if not already there\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# 4. Change the working directory\n",
    "os.chdir(project_root)\n",
    "\n",
    "print(f\"Current Working Directory: {os.getcwd()}\")\n",
    "print(\"Python Path updated. You can now import from 'src'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff96a80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# Create a main project folder in your Drive\n",
    "project_path = \"/content/drive/MyDrive/OmniScale-Optimizer\"\n",
    "if not os.path.exists(project_path):\n",
    "    os.makedirs(project_path)\n",
    "    print(f\"Created project folder at {project_path}\")\n",
    "\n",
    "# Change the current working directory to the project folder\n",
    "os.chdir(project_path)\n",
    "\n",
    "folders = [\n",
    "    \"data/raw\",\n",
    "    \"data/processed\",\n",
    "    \"notebooks\",\n",
    "    \"src/parser\",\n",
    "    \"src/models\",\n",
    "    \"src/optimizer/cpp_core\",\n",
    "    \"src/distributed\",\n",
    "    \"tests\",\n",
    "    \"scripts\"\n",
    "]\n",
    "\n",
    "for folder in folders:\n",
    "    path = os.path.join(project_path, folder)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# Create empty __init__.py files to make them Python modules\n",
    "\n",
    "init_files = [\n",
    "    \"src/__init__.py\",\n",
    "\n",
    "    \"src/parser/__init__.py\",\n",
    "    \"src/models/__init__.py\",\n",
    "    \"src/optimizer/__init__.py\",\n",
    "    \"src/distributed/__init__.py\"\n",
    "]\n",
    "\n",
    "for file in init_files:\n",
    "    with open(os.path.join(project_path, file), 'w') as f:\n",
    "        pass\n",
    "\n",
    "print(\"Project structure created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20d2b381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/parser/stream_parser.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/parser/stream_parser.py\n",
    "import json\n",
    "\n",
    "def stream_amazon_data(file_path):\n",
    "    \"\"\"A generator that yields one row at a time (Phase 1: Parsing)\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Stream Parser Module Initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78791b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of /content/drive/MyDrive/OmniScale-Optimizer:\n",
      "  data\n",
      "  notebooks\n",
      "  src\n",
      "  tests\n",
      "  scripts\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if project folder exists and list its contents\n",
    "project_path = \"/content/drive/MyDrive/OmniScale-Optimizer\"\n",
    "if os.path.exists(project_path):\n",
    "    print(f\"Contents of {project_path}:\")\n",
    "    for item in os.listdir(project_path):\n",
    "        print(f\"  {item}\")\n",
    "else:\n",
    "    print(\"Folder doesn't exist yet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4c6269f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 20\n",
      "drwx------ 2 root root 4096 Feb 20 03:48 data\n",
      "drwx------ 2 root root 4096 Feb 20 03:48 notebooks\n",
      "drwx------ 2 root root 4096 Feb 20 03:48 scripts\n",
      "drwx------ 3 root root 4096 Feb 20 03:49 src\n",
      "drwx------ 2 root root 4096 Feb 20 03:48 tests\n"
     ]
    }
   ],
   "source": [
    "!ls -la /content/drive/MyDrive/OmniScale-Optimizer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7da959d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /content/drive/MyDrive/OmniScale-Optimizer\n",
      "Contents: ['data', 'notebooks', 'src', 'tests', 'scripts']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current directory:\", os.getcwd())\n",
    "print(\"Contents:\", os.listdir('.'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e4a2bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pybind11\n",
      "  Downloading pybind11-3.0.2-py3-none-any.whl.metadata (10 kB)\n",
      "Downloading pybind11-3.0.2-py3-none-any.whl (310 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.2/310.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pybind11\n",
      "Successfully installed pybind11-3.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pybind11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4fbb078b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Current Working Directory: /content/drive/MyDrive/OmniScale-Optimizer\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 1. Mount Drive (Look for a notification/popup in your browser window if it hangs)\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 2. Define the project path\n",
    "project_root = '/content/drive/MyDrive/OmniScale-Optimizer'\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# 3. Change directory so relative imports work\n",
    "os.chdir(project_root)\n",
    "\n",
    "print(f\"Current Working Directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bce6d22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /content/drive/MyDrive/OmniScale-Optimizer/data/raw\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Make sure your drive is mounted first!\n",
    "raw_data_path = \"/content/drive/MyDrive/OmniScale-Optimizer/data/raw\"\n",
    "os.chdir(raw_data_path)\n",
    "print(f\"Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0741aa75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-02-20 04:28:25--  http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Electronics_5.json.gz\n",
      "Resolving snap.stanford.edu (snap.stanford.edu)... 171.64.75.80\n",
      "Connecting to snap.stanford.edu (snap.stanford.edu)|171.64.75.80|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 495854086 (473M) [application/x-gzip]\n",
      "Saving to: ‘reviews_Electronics_5.json.gz.1’\n",
      "\n",
      "reviews_Electronics 100%[===================>] 472.88M  36.8MB/s    in 16s     \n",
      "\n",
      "2026-02-20 04:28:42 (29.1 MB/s) - ‘reviews_Electronics_5.json.gz.1’ saved [495854086/495854086]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the 5-core Electronics dataset (approx 400MB compressed, 1.5GB uncompressed)\n",
    "!wget http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Electronics_5.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d1a1ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This unzips the file into a .json file\n",
    "!gunzip reviews_Electronics_5.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "acdcb814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1.9G\n",
      "-rw------- 1 root root 1.4G Apr 26  2016 reviews_Electronics_5.json\n",
      "-rw------- 1 root root 473M Apr 26  2016 reviews_Electronics_5.json.gz.1\n"
     ]
    }
   ],
   "source": [
    "!ls -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7eef2f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reviews: 1689188\n",
      "Unique reviewers: 192403\n",
      "Potential duplicates: 1496785\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "# Count records in JSON file\n",
    "count = 0\n",
    "seen = set()\n",
    "duplicates = 0\n",
    "\n",
    "with open('/content/drive/MyDrive/OmniScale-Optimizer/data/raw/reviews_Electronics_5.json', 'r') as f:\n",
    "    for line in f:\n",
    "        count += 1\n",
    "        if line.strip():\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                review_id = data.get('reviewerID', '')\n",
    "                if review_id in seen:\n",
    "                    duplicates += 1\n",
    "                seen.add(review_id)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "print(f\"Total reviews: {count}\")\n",
    "print(f\"Unique reviewers: {len(seen)}\")\n",
    "print(f\"Potential duplicates: {duplicates}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "219da83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True duplicates (same user + same product): 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "# Check for true duplicates (same reviewerID + asin)\n",
    "seen_pairs = set()\n",
    "true_duplicates = 0\n",
    "\n",
    "with open('/content/drive/MyDrive/OmniScale-Optimizer/data/raw/reviews_Electronics_5.json', 'r') as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                pair = (data.get('reviewerID', ''), data.get('asin', ''))\n",
    "                if pair[0] and pair[1]:  # both exist\n",
    "                    if pair in seen_pairs:\n",
    "                        true_duplicates += 1\n",
    "                    seen_pairs.add(pair)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "print(f\"True duplicates (same user + same product): {true_duplicates}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6daa5902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available in stream_parser.py:\n",
      "['json', 'stream_amazon_data']\n"
     ]
    }
   ],
   "source": [
    "# First, let's see what's in your module\n",
    "import sys\n",
    "sys.path.insert(0, '/content/drive/MyDrive/OmniScale-Optimizer')\n",
    "\n",
    "# Check what's exported\n",
    "import src.parser.stream_parser as sp_module\n",
    "print(\"Available in stream_parser.py:\")\n",
    "print([item for item in dir(sp_module) if not item.startswith('_')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "468d4db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/parser/feature_miner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/parser/feature_miner.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from src.parser.stream_parser import stream_amazon_data\n",
    "\n",
    "class FeatureMiner:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def extract_interactions(self, limit=100000):\n",
    "        \"\"\"Extracts User-Item-Rating and generates random Lat/Lon for users\"\"\"\n",
    "        data = []\n",
    "        gen = stream_amazon_data(self.file_path)\n",
    "        \n",
    "        for i, record in enumerate(gen):\n",
    "            if i >= limit: break\n",
    "            data.append({\n",
    "                'user_id': record.get('reviewerID'),\n",
    "                'item_id': record.get('asin'),\n",
    "                'rating': record.get('overall'),\n",
    "                'lat': np.random.uniform(30, 45),\n",
    "                'lon': np.random.uniform(-120, -70)\n",
    "            })\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "    def manual_kmeans(self, points, k, max_iters=100):\n",
    "        \"\"\"K-Means++ implementation from scratch (Data Mining)\"\"\"\n",
    "        centroids = points[np.random.choice(points.shape[0], k, replace=False)]\n",
    "        for _ in range(max_iters):\n",
    "            distances = np.linalg.norm(points[:, np.newaxis] - centroids, axis=2)\n",
    "            labels = np.argmin(distances, axis=1)\n",
    "            new_centroids = np.array([\n",
    "                points[labels == i].mean(axis=0) if len(points[labels == i]) > 0 else centroids[i]\n",
    "                for i in range(k)\n",
    "            ])\n",
    "            if np.allclose(centroids, new_centroids):\n",
    "                break\n",
    "            centroids = new_centroids\n",
    "        return centroids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "88f28d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 100000 interactions.\n",
      "Data saved to Drive!\n"
     ]
    }
   ],
   "source": [
    "from src.parser.feature_miner import FeatureMiner\n",
    "\n",
    "# Path to the dataset you downloaded to your Drive\n",
    "raw_json = \"/content/drive/MyDrive/OmniScale-Optimizer/data/raw/reviews_Electronics_5.json\"\n",
    "\n",
    "# Initialize and Process\n",
    "miner = FeatureMiner(raw_json)\n",
    "df_interactions = miner.extract_interactions(limit=100000)\n",
    "\n",
    "print(f\"Extracted {len(df_interactions)} interactions.\")\n",
    "\n",
    "# Cluster user locations (Lat/Lon) into 10 Delivery Zones\n",
    "points = df_interactions[['lat', 'lon']].values\n",
    "centroids, labels = miner.manual_kmeans(points, k=10)\n",
    "\n",
    "# Add the Zone Label to our dataframe\n",
    "df_interactions['delivery_zone'] = labels\n",
    "\n",
    "# Save processed data to Drive to save local 3GB space\n",
    "df_interactions.to_csv(\"/content/drive/MyDrive/OmniScale-Optimizer/data/processed/clean_data.csv\", index=False)\n",
    "print(\"Data saved to Drive!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e62b901f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worked! Extracted 100 rows.\n"
     ]
    }
   ],
   "source": [
    "from src.parser.feature_miner import FeatureMiner\n",
    "\n",
    "# Initialize with your data path\n",
    "raw_json = \"/content/drive/MyDrive/OmniScale-Optimizer/data/raw/reviews_Electronics_5.json\"\n",
    "miner = FeatureMiner(raw_json)\n",
    "\n",
    "# Test it\n",
    "df = miner.extract_interactions(limit=100)\n",
    "print(f\"Worked! Extracted {len(df)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0999297c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 60814 users and 4181 items.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Load the data we mined in Phase 1\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/OmniScale-Optimizer/data/processed/clean_data.csv\")\n",
    "\n",
    "# 2. Map IDs to Integers\n",
    "df['user_idx'] = df['user_id'].astype('category').cat.codes\n",
    "df['item_idx'] = df['item_id'].astype('category').cat.codes\n",
    "\n",
    "# 3. Get total counts (needed for the model's Embedding layers)\n",
    "num_users = df['user_idx'].nunique()\n",
    "num_items = df['item_idx'].nunique()\n",
    "\n",
    "print(f\"Dataset has {num_users} users and {num_items} items.\")\n",
    "\n",
    "# 4. Split into Training and Testing\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the counts for later use\n",
    "with open(\"/content/drive/MyDrive/OmniScale-Optimizer/data/processed/metadata.txt\", \"w\") as f:\n",
    "    f.write(f\"{num_users},{num_items}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "af7cb572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/models/ncf_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/models/ncf_model.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NCFModel(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embed_size=32):\n",
    "        super(NCFModel, self).__init__()\n",
    "        \n",
    "        # 1. Embedding Layers (Algorithms & DS: O(1) Lookup Tables)\n",
    "        self.user_embed = nn.Embedding(num_users, embed_size)\n",
    "        self.item_embed = nn.Embedding(num_items, embed_size)\n",
    "        \n",
    "        # 2. Neural Network Layers (MLP)\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(embed_size * 2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)  # Output is a single \"Prediction Score\"\n",
    "        )\n",
    "        \n",
    "    def forward(self, user_indices, item_indices):\n",
    "        # Lookup embeddings\n",
    "        u_emb = self.user_embed(user_indices)\n",
    "        i_emb = self.item_embed(item_indices)\n",
    "        \n",
    "        # Concatenate user and item vectors\n",
    "        x = torch.cat([u_emb, i_emb], dim=-1)\n",
    "        \n",
    "        # Pass through the neural network\n",
    "        prediction = self.fc_layers(x)\n",
    "        return prediction.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "620dd638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Epoch 1, Loss: 18.2124\n",
      "Epoch 2, Loss: 17.9507\n",
      "Epoch 3, Loss: 17.6857\n",
      "Epoch 4, Loss: 17.4165\n",
      "Epoch 5, Loss: 17.1423\n",
      "Model saved to Drive!\n"
     ]
    }
   ],
   "source": [
    "from src.models.ncf_model import NCFModel\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# 1. Setup Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 2. Initialize Model\n",
    "model = NCFModel(num_users, num_items).to(device)\n",
    "\n",
    "# 3. Define Optimizer (Numerical Optimization) and Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 4. Prepare Tensors\n",
    "user_train = torch.LongTensor(train['user_idx'].values).to(device)\n",
    "item_train = torch.LongTensor(train['item_idx'].values).to(device)\n",
    "ratings_train = torch.FloatTensor(train['rating'].values).to(device)\n",
    "\n",
    "# 5. Training Loop\n",
    "model.train()\n",
    "for epoch in range(5):  # 5 Epochs for testing\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward Pass\n",
    "    outputs = model(user_train, item_train)\n",
    "    loss = criterion(outputs, ratings_train)\n",
    "    \n",
    "    # Backward Pass (Numerical Opt: Gradient Calculation)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 6. Save Model Weights (Persistent Storage)\n",
    "torch.save(model.state_dict(), \"/content/drive/MyDrive/OmniScale-Optimizer/data/processed/ncf_model.pth\")\n",
    "print(\"Model saved to Drive!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "eaa105a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/optimizer/solver.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/optimizer/solver.py\n",
    "import numpy as np\n",
    "\n",
    "class LogisticsOptimizer:\n",
    "    def __init__(self, warehouse_locations, warehouse_capacities):\n",
    "        \"\"\"\n",
    "        warehouse_locations: Lat/Lon of centroids from Phase 1\n",
    "        warehouse_capacities: List of max orders each warehouse can take\n",
    "        \"\"\"\n",
    "        self.warehouses = warehouse_locations\n",
    "        self.capacities = warehouse_capacities\n",
    "\n",
    "    def calculate_distance(self, p1, p2):\n",
    "        \"\"\"Euclidean distance (HPC Optimization target later!)\"\"\"\n",
    "        return np.linalg.norm(p1 - p2)\n",
    "\n",
    "    def assign_orders(self, user_locations):\n",
    "        \"\"\"\n",
    "        Numerical Optimization: Assign users to warehouses to minimize distance\n",
    "        under capacity constraints.\n",
    "        \"\"\"\n",
    "        num_users = len(user_locations)\n",
    "        assignments = np.full(num_users, -1)\n",
    "        current_usage = np.zeros(len(self.warehouses))\n",
    "\n",
    "        # 1. Create a cost matrix (Distance from every user to every warehouse)\n",
    "        # This is O(N*M) - we will optimize this in C++ in Phase 4!\n",
    "        for i in range(num_users):\n",
    "            user_pos = user_locations[i]\n",
    "            \n",
    "            # Find distances to all warehouses\n",
    "            costs = [self.calculate_distance(user_pos, w) for w in self.warehouses]\n",
    "            \n",
    "            # Sort warehouses by distance (closest first)\n",
    "            preferred_warehouses = np.argsort(costs)\n",
    "            \n",
    "            # 2. Assignment Logic with Constraint Checking\n",
    "            for w_idx in preferred_warehouses:\n",
    "                if current_usage[w_idx] < self.capacities[w_idx]:\n",
    "                    assignments[i] = w_idx\n",
    "                    current_usage[w_idx] += 1\n",
    "                    break\n",
    "                    \n",
    "        return assignments, current_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c86de30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Orders: 100000\n",
      "Capacity per Warehouse: 11000\n",
      "Solving assignment optimization...\n",
      "Optimization Complete!\n",
      "Orders unassigned (due to capacity): 0\n",
      "Warehouse Usage: [10129.  9803. 10362.  8851. 10026. 10144. 10774.  9049. 10625. 10237.]\n"
     ]
    }
   ],
   "source": [
    "from src.optimizer.solver import LogisticsOptimizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load your processed data\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/OmniScale-Optimizer/data/processed/clean_data.csv\")\n",
    "\n",
    "# 2. Get Warehouse Locations (The centroids from Phase 1)\n",
    "# For this demo, we'll re-calculate them or use unique zones\n",
    "warehouse_locations = df.groupby('delivery_zone')[['lat', 'lon']].mean().values\n",
    "num_warehouses = len(warehouse_locations)\n",
    "\n",
    "# 3. Define Constraints (Amazon-style Capacity Limits)\n",
    "# Each warehouse can only handle 12% of the total orders (making it a tight constraint)\n",
    "total_orders = len(df)\n",
    "capacity_per_warehouse = int((total_orders / num_warehouses) * 1.1) \n",
    "capacities = [capacity_per_warehouse] * num_warehouses\n",
    "\n",
    "print(f\"Total Orders: {total_orders}\")\n",
    "print(f\"Capacity per Warehouse: {capacity_per_warehouse}\")\n",
    "\n",
    "# 4. Initialize and Run the Optimizer\n",
    "optimizer = LogisticsOptimizer(warehouse_locations, capacities)\n",
    "user_coords = df[['lat', 'lon']].values\n",
    "\n",
    "print(\"Solving assignment optimization...\")\n",
    "assignments, final_usage = optimizer.assign_orders(user_coords)\n",
    "\n",
    "# 5. Analyze Results\n",
    "df['assigned_warehouse'] = assignments\n",
    "unassigned = np.sum(assignments == -1)\n",
    "\n",
    "print(f\"Optimization Complete!\")\n",
    "print(f\"Orders unassigned (due to capacity): {unassigned}\")\n",
    "print(f\"Warehouse Usage: {final_usage}\")\n",
    "\n",
    "# Save the final optimized plan\n",
    "df.to_csv(\"/content/drive/MyDrive/OmniScale-Optimizer/data/processed/final_shipping_plan.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca5c7f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Current Directory: /content/drive/MyDrive/OmniScale-Optimizer\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "# 1. Mount Drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# 2. Define and Move to Project Root\n",
    "project_root = \"/content/drive/MyDrive/OmniScale-Optimizer\"\n",
    "os.chdir(project_root)\n",
    "\n",
    "# 3. Force-create the specific subfolder (just in case)\n",
    "os.makedirs(\"src/optimizer/cpp_core\", exist_ok=True)\n",
    "\n",
    "print(f\"Current Directory: {os.getcwd()}\")\n",
    "# It should print: /content/drive/MyDrive/OmniScale-Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d713cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/optimizer/cpp_core/optimizer.cpp\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/optimizer/cpp_core/optimizer.cpp\n",
    "#include <pybind11/pybind11.h>\n",
    "#include <pybind11/numpy.h>\n",
    "#include <vector>\n",
    "#include <cmath>\n",
    "#include <algorithm>\n",
    "#include <omp.h>\n",
    "\n",
    "namespace py = pybind11;\n",
    "\n",
    "py::array_t<int> fast_assign(py::array_t<double> user_locs, \n",
    "                            py::array_t<double> warehouse_locs, \n",
    "                            py::array_t<int> capacities) {\n",
    "    \n",
    "    auto users = user_locs.unchecked<2>();\n",
    "    auto warehouses = warehouse_locs.unchecked<2>();\n",
    "    \n",
    "    int n_users = users.shape(0);\n",
    "    int n_warehouses = warehouses.shape(0);\n",
    "    \n",
    "    py::array_t<int> assignments({n_users});\n",
    "    auto assign_ptr = assignments.mutable_unchecked<1>();\n",
    "\n",
    "    #pragma omp parallel for\n",
    "    for (int i = 0; i < n_users; i++) {\n",
    "        double min_dist = 1e18;\n",
    "        int best_w = -1;\n",
    "\n",
    "        for (int j = 0; j < n_warehouses; j++) {\n",
    "            double dx = users(i, 0) - warehouses(j, 0);\n",
    "            double dy = users(i, 1) - warehouses(j, 1);\n",
    "            double dist = std::sqrt(dx*dx + dy*dy);\n",
    "            \n",
    "            if (dist < min_dist) {\n",
    "                min_dist = dist;\n",
    "                best_w = j;\n",
    "            }\n",
    "        }\n",
    "        assign_ptr(i) = best_w;\n",
    "    }\n",
    "    return assignments;\n",
    "}\n",
    "\n",
    "PYBIND11_MODULE(fast_optimizer, m) {\n",
    "    m.def(\"fast_assign\", &fast_assign, \"High-performance order assignment\");\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "096e765e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File created successfully!\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"src/optimizer/cpp_core/optimizer.cpp\"):\n",
    "    print(\"✅ File created successfully!\")\n",
    "else:\n",
    "    print(\"❌ File still missing. Check if your Drive has space or if the folder name has a typo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfa7fb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybind11\n",
    "\n",
    "# Get include path\n",
    "inc = pybind11.get_include()\n",
    "\n",
    "# Compile the .so file\n",
    "!g++ -O3 -fopenmp -Wall -shared -std=c++11 -fPIC \\\n",
    "    $(python3 -m pybind11 --includes) \\\n",
    "    src/optimizer/cpp_core/optimizer.cpp \\\n",
    "    -o optimizer$(python3-config --extension-suffix)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd480f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/distributed/map_reduce_ops.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/distributed/map_reduce_ops.py\n",
    "import numpy as np\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Ensure the C++ library can be found by workers\n",
    "sys.path.append(\"/content/drive/MyDrive/OmniScale-Optimizer/src/optimizer/cpp_core\")\n",
    "import fast_optimizer\n",
    "\n",
    "def worker_task(chunk_data, warehouse_coords, caps):\n",
    "    \"\"\"\n",
    "    The 'Map' Step: Each worker processes a slice of the data.\n",
    "    In a real system, this would be a separate EC2 instance.\n",
    "    \"\"\"\n",
    "    # Use our HPC C++ kernel inside the worker\n",
    "    assignments = fast_optimizer.fast_assign(chunk_data, warehouse_coords, caps)\n",
    "    \n",
    "    # Calculate local metrics to return to the master\n",
    "    local_count = len(assignments)\n",
    "    local_usage = np.bincount(assignments, minlength=len(warehouse_coords))\n",
    "    \n",
    "    return {\n",
    "        \"assignments\": assignments,\n",
    "        \"usage\": local_usage,\n",
    "        \"count\": local_count\n",
    "    }\n",
    "\n",
    "def run_distributed_optimizer(df, warehouse_coords, caps, num_workers=4):\n",
    "    \"\"\"\n",
    "    The 'Master' Logic: Orchestrates the distribution and aggregation.\n",
    "    \"\"\"\n",
    "    # 1. Split data into shards (Data Partitioning)\n",
    "    user_coords = df[['lat', 'lon']].values.astype(np.float64)\n",
    "    chunks = np.array_split(user_coords, num_workers)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # 2. Parallel Execution (Simulating a Distributed Cluster)\n",
    "    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = [executor.submit(worker_task, chunk, warehouse_coords, caps) for chunk in chunks]\n",
    "        for future in futures:\n",
    "            results.append(future.result())\n",
    "            \n",
    "    # 3. The 'Reduce' Step: Aggregate results from all workers\n",
    "    total_usage = np.zeros(len(warehouse_coords))\n",
    "    all_assignments = []\n",
    "    \n",
    "    for res in results:\n",
    "        total_usage += res[\"usage\"]\n",
    "        all_assignments.extend(res[\"assignments\"])\n",
    "        \n",
    "    return all_assignments, total_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06bd47db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling with paths...\n",
      "✅ Compilation Successful! .so file created.\n"
     ]
    }
   ],
   "source": [
    "import pybind11\n",
    "import sys\n",
    "\n",
    "# Get the include paths for both pybind11 and Python itself\n",
    "pybind_inc = pybind11.get_include()\n",
    "python_inc = !python3-config --includes\n",
    "\n",
    "# Join the list into a string\n",
    "python_inc_str = \" \".join(python_inc)\n",
    "\n",
    "print(\"Compiling with paths...\")\n",
    "!g++ -O3 -Wall -shared -std=c++11 -fPIC -fopenmp \\\n",
    "    {python_inc_str} \\\n",
    "    -I{pybind_inc} \\\n",
    "    src/optimizer/cpp_core/optimizer.cpp \\\n",
    "    -o src/optimizer/cpp_core/fast_optimizer.so\n",
    "\n",
    "import os\n",
    "if os.path.exists(\"src/optimizer/cpp_core/fast_optimizer.so\"):\n",
    "    print(\"✅ Compilation Successful! .so file created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "049501e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data loaded: 100000 rows.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 1. Reload the data from Phase 1\n",
    "data_path = \"/content/drive/MyDrive/OmniScale-Optimizer/data/processed/clean_data.csv\"\n",
    "\n",
    "if os.path.exists(data_path):\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"✅ Data loaded: {len(df)} rows.\")\n",
    "else:\n",
    "    print(\"❌ Error: clean_data.csv not found in Drive. Did you run Phase 1?\")\n",
    "\n",
    "# 2. Re-calculate or load the warehouse locations (centroids)\n",
    "# Since centroids were derived from the zones in Phase 1:\n",
    "warehouse_locations = df.groupby('delivery_zone')[['lat', 'lon']].mean().values\n",
    "num_warehouses = len(warehouse_locations)\n",
    "total_orders = len(df)\n",
    "capacity_per_warehouse = int((total_orders / num_warehouses) * 1.1) \n",
    "capacities = [capacity_per_warehouse] * num_warehouses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97d462fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Distributed MapReduce Simulation...\n",
      "✅ Success! Distributed Execution Time: 0.0606 seconds\n",
      "Total Orders: 100000\n"
     ]
    }
   ],
   "source": [
    "from src.distributed.map_reduce_ops import run_distributed_optimizer\n",
    "import time\n",
    "\n",
    "# Ensure inputs are correctly typed for C++\n",
    "warehouse_coords = warehouse_locations.astype(np.float64)\n",
    "caps = np.array(capacities).astype(np.int32)\n",
    "\n",
    "print(\"Starting Distributed MapReduce Simulation...\")\n",
    "start = time.time()\n",
    "\n",
    "# We use 2 workers to match Colab's standard CPU count\n",
    "dist_assignments, dist_usage = run_distributed_optimizer(\n",
    "    df, \n",
    "    warehouse_coords, \n",
    "    caps, \n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "dist_time = time.time() - start\n",
    "print(f\"✅ Success! Distributed Execution Time: {dist_time:.4f} seconds\")\n",
    "print(f\"Total Orders: {len(dist_assignments)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
